{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Regression on Rossmann Sales\n",
    "\n",
    "This notebook implements supervised regression models on the Rossmann sales dataset. We start with a simple baseline, then train several regression models (linear, polynomial, Ridge, Random Forest) using the processed data from `Data/processed/train_clean.csv` and compare their performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup\n",
    "\n",
    "We import common data science libraries (`pandas`, `numpy`, `matplotlib`, `seaborn`) for data handling and visualization, and scikit-learn tools for splitting the data, preprocessing features, building pipelines, and training regression models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed Rossmann training data\n",
    "\n",
    "We load the cleaned and enriched training dataset `train_clean.csv` that was produced by the ETL pipeline. The `Date` column is parsed as a datetime so that we can easily derive calendar-based features (e.g. year, month, week of year) used later in the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path().resolve().parents[0] if len(Path().resolve().parents) > 1 else Path().resolve()\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"Data\" / \"processed\"\n",
    "\n",
    "dtype_map = {\"StateHoliday\": \"string\", \"PromoInterval\": \"string\"}\n",
    "\n",
    "train = pd.read_csv(DATA_PROCESSED / \"train_clean.csv\", dtype=dtype_map, parse_dates=[\"Date\"])\n",
    "print(\"Shape of training data:\", train.shape)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering and feature selection\n",
    "\n",
    "We define the supervised learning task as predicting daily `Sales` for each store. From the `Date` we create additional time features (year, month, day, week of year). We then select a set of numeric features (e.g. `Store`, `DayOfWeek`, `Promo`, competition and promo indicators, calendar features) and categorical features (e.g. `StateHoliday`, `StoreType`, `Assortment`, `State`) that are likely to influence sales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: date parts\n",
    "train[\"Year\"] = train[\"Date\"].dt.year\n",
    "train[\"Month\"] = train[\"Date\"].dt.month\n",
    "train[\"Day\"] = train[\"Date\"].dt.day\n",
    "train[\"WeekOfYear\"] = train[\"Date\"].dt.isocalendar().week.astype(int)\n",
    "\n",
    "TARGET = \"Sales\"\n",
    "\n",
    "numeric_features = [\n",
    "    \"Store\",\n",
    "    \"DayOfWeek\",\n",
    "    \"Promo\",\n",
    "    \"SchoolHoliday\",\n",
    "    \"CompetitionDistance\",\n",
    "    \"CompetitionOpenSinceMonth\",\n",
    "    \"CompetitionOpenSinceYear\",\n",
    "    \"Promo2\",\n",
    "    \"Promo2SinceWeek\",\n",
    "    \"Promo2SinceYear\",\n",
    "    \"Year\",\n",
    "    \"Month\",\n",
    "    \"Day\",\n",
    "    \"WeekOfYear\",\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"StateHoliday\",\n",
    "    \"StoreType\",\n",
    "    \"Assortment\",\n",
    "    \"State\",\n",
    "]\n",
    "\n",
    "feature_cols = numeric_features + categorical_features\n",
    "\n",
    "X = train[feature_cols].copy()\n",
    "y = train[TARGET].values\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/validation split\n",
    "\n",
    "We split the data into training and validation sets (80/20). The models are fitted only on the training data and evaluated on the held-out validation data. This gives us an unbiased estimate of how well each model generalizes. For a true time-series forecasting setup we would use a time-based split, but here we follow the simpler random split as in the lecture example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split (simple random split)\n",
    "# Note: for true time-series forecasting, a time-based split would be more appropriate.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline\n",
    "\n",
    "Before training models we build a preprocessing pipeline. Numeric features are imputed (median) and scaled, while categorical features are imputed (most frequent value) and one-hot encoded. Using a `ColumnTransformer` inside a scikit-learn `Pipeline` ensures that the exact same preprocessing is applied during training and when making predictions, avoiding data leakage and keeping the workflow reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: impute missing values, scale numeric features, one-hot encode categoricals\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "preprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics and baseline model\n",
    "\n",
    "To evaluate models we use three common regression metrics:\n",
    "- **MAE (Mean Absolute Error)**: average absolute difference between predicted and true sales.\n",
    "- **RMSE (Root Mean Squared Error)**: penalizes larger errors more strongly than MAE.\n",
    "- **$R^2$ (coefficient of determination)**: proportion of variance in sales explained by the model.\n",
    "\n",
    "As a simple baseline we always predict the mean `Sales` from the training set. Any useful model should significantly improve over this baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics(y_true, y_pred, label=\"model\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)  # older sklearn: no 'squared' argument\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{label}: MAE={mae:,.2f}, RMSE={rmse:,.2f}, R^2={r2:.4f}\")\n",
    "\n",
    "# Baseline model: predict mean of training Sales\n",
    "baseline_pred = np.full_like(y_test, fill_value=y_train.mean(), dtype=float)\n",
    "regression_metrics(y_test, baseline_pred, label=\"Baseline (mean)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model\n",
    "\n",
    "We first fit a standard linear regression model, which assumes a linear relationship between the (preprocessed) features and `Sales`. This model is simple, interpretable, and provides a strong baseline to compare more complex models against.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression model\n",
    "linear_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"regressor\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "regression_metrics(y_test, y_pred_linear, label=\"Linear Regression\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression model\n",
    "\n",
    "To capture non-linear relationships and interactions between features, we extend the linear model with polynomial features of degree 2. This allows the model to fit more complex patterns in the data, at the cost of higher risk of overfitting and increased computational cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression (degree=2) on preprocessed features\n",
    "poly_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        (\"regressor\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "poly_model.fit(X_train, y_train)\n",
    "y_pred_poly = poly_model.predict(X_test)\n",
    "regression_metrics(y_test, y_pred_poly, label=\"Polynomial Regression (degree=2)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic plots: true vs predicted\n",
    "\n",
    "We visualize the relationship between true and predicted `Sales` for the linear and polynomial models. Points close to the diagonal line indicate good predictions. Systematic deviations from the diagonal can reveal underfitting, overfitting, or heteroscedasticity (errors growing with sales level).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
    "source": [
    "def plot_true_vs_pred(y_true, y_pred, title):\n",
    "    plt.figure()\n",
    "    plt.scatter(y_true, y_pred, alpha=0.3)\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect prediction')\n",
    "    plt.xlabel(\"True Sales\")\n",
    "    plt.ylabel(\"Predicted Sales\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_true_vs_pred(y_test, y_pred_linear, \"Linear Regression: True vs Predicted Sales\")\n",
    "plot_true_vs_pred(y_test, y_pred_poly, \"Polynomial Regression: True vs Predicted Sales\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced models: Ridge and Random Forest\n",
    "\n",
    "To go beyond plain linear and polynomial regression, we train two additional models:\n",
    "- **Ridge regression**: a linear model with L2 regularization, which can reduce overfitting by shrinking coefficients and handling multicollinearity.\n",
    "- **Random Forest Regressor**: an ensemble of decision trees that captures complex non-linear relationships and interactions without requiring explicit feature engineering.\n",
    "\n",
    "We keep the hyperparameters relatively simple (e.g. `alpha=1.0` for Ridge, `n_estimators=100`, `max_depth=10` for Random Forest) to mirror the level of experimentation expected in the course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "models = {}\n",
    "\n",
    "# Reuse baseline, linear, and polynomial predictions\n",
    "models[\"Baseline (mean)\"] = (y_test, baseline_pred)\n",
    "models[\"Linear Regression\"] = (y_test, y_pred_linear)\n",
    "models[\"Polynomial (deg=2)\"] = (y_test, y_pred_poly)\n",
    "\n",
    "# Ridge regression\n",
    "ridge_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"regressor\", Ridge(alpha=1.0)),\n",
    "    ]\n",
    ")\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "models[\"Ridge (alpha=1.0)\"] = (y_test, y_pred_ridge)\n",
    "\n",
    "# Random Forest regression (simple configuration)\n",
    "rf_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"regressor\", RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "models[\"RandomForest (100, depth=10)\"] = (y_test, y_pred_rf)\n",
    "\n",
    "print(\"Trained advanced models: Ridge and RandomForest.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric comparison table\n",
    "\n",
    "For each model (baseline, linear, polynomial, Ridge, Random Forest) we compute the same set of metrics (MAE, RMSE, $R^2$) on the validation set and collect them in a single table. This makes it easy to compare models quantitatively and justify which one performs best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for name, (yt, yp) in models.items():\n",
    "    mae = mean_absolute_error(yt, yp)\n",
    "    mse = mean_squared_error(yt, yp)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(yt, yp)\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows).sort_values(\"RMSE\")\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison plot\n",
    "\n",
    "Finally, we plot the RMSE of each model as a bar chart. Lower RMSE means better predictive performance. This visualization is useful for the presentation to clearly communicate which supervised model works best for the Rossmann sales prediction task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x=\"Model\", y=\"RMSE\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Model Comparison: RMSE on Validation Set\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
