{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Configure plots\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"--- Initializing Clustering Environment ---\")\n",
    "\n",
    "## 1. Data Loading and Preparation ðŸ’¾\n",
    "PROJECT_ROOT = Path().resolve().parents[0] if len(Path().resolve().parents) > 1 else Path().resolve()\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"Data\" / \"processed\"\n",
    "\n",
    "# Fallback path check\n",
    "if not (DATA_PROCESSED / \"train_clean.csv\").exists():\n",
    "    print(f\"ERROR: train_clean.csv not found at {DATA_PROCESSED}. Checking current directory.\")\n",
    "    DATA_PROCESSED = Path().resolve()\n",
    "\n",
    "# Load cleaned data\n",
    "dtype_map = {\"StateHoliday\": \"string\", \"PromoInterval\": \"string\"}\n",
    "try:\n",
    "    train = pd.read_csv(DATA_PROCESSED / \"train_clean.csv\", dtype=dtype_map)\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nFATAL ERROR: Could not load 'train_clean.csv'. Check your directory structure.\")\n",
    "    exit()\n",
    "\n",
    "# Ensure Date column is datetime\n",
    "train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
    "print(\"Loaded processed training dataset.\")\n",
    "\n",
    "\n",
    "# --- Apply Clipping (1%-99% percentile) for Outlier Mitigation ---\n",
    "def clip_percentile(series, lower_q=0.01, upper_q=0.99):\n",
    "    lower = series.quantile(lower_q)\n",
    "    upper = series.quantile(upper_q)\n",
    "    return series.clip(lower, upper)\n",
    "\n",
    "\n",
    "numeric_cols = train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cols_to_clip = [col for col in numeric_cols if\n",
    "                col not in [\"Store\", \"Date\", \"DayOfWeek\", \"Year\", \"Month\", \"Day\", \"WeekOfYear\"]]\n",
    "\n",
    "train_clipped_p99 = train.copy()\n",
    "for col in cols_to_clip:\n",
    "    train_clipped_p99[col] = clip_percentile(train_clipped_p99[col])\n",
    "print(\"Applied 1%-99% percentile clipping to key numerical features.\")"
   ],
   "id": "a051c04ab3138763",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## 2. Feature Aggregation, Imputation, and Scaling ðŸ› ï¸\n",
    "\n",
    "# Aggregating key operational metrics per Store, using clipped data\n",
    "store_data = (\n",
    "    train_clipped_p99.groupby(\"Store\")\n",
    "    .agg(\n",
    "        Sales=(\"Sales\", \"mean\"),\n",
    "        Customers=(\"Customers\", \"mean\"),\n",
    "        CompetitionDistance=(\"CompetitionDistance\", \"mean\"),\n",
    "        PromoUsage=(\"Promo\", \"mean\"),\n",
    "        StoreType=(\"StoreType\", \"first\"),\n",
    "        Assortment=(\"Assortment\", \"first\"),\n",
    "        State=(\"State\", \"first\"),\n",
    "        Promo2=(\"Promo2\", \"first\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate SalesPerCustomer after aggregation\n",
    "store_data[\"SalesPerCustomer\"] = store_data[\"Sales\"] / store_data[\"Customers\"]\n",
    "store_data[\"SalesPerCustomer\"] = store_data[\"SalesPerCustomer\"].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Features selected for clustering (must be numerical)\n",
    "clustering_features = [\n",
    "    \"Sales\",\n",
    "    \"Customers\",\n",
    "    \"SalesPerCustomer\",\n",
    "    \"CompetitionDistance\",\n",
    "    \"PromoUsage\",\n",
    "    \"Promo2\",\n",
    "]\n",
    "\n",
    "# Impute NaNs using the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "store_data[clustering_features] = imputer.fit_transform(store_data[clustering_features])\n",
    "print(\"NaNs imputed using median strategy.\")\n",
    "\n",
    "# Scaling Numerical Features\n",
    "scaler = StandardScaler()\n",
    "store_data_scaled = scaler.fit_transform(store_data[clustering_features])\n",
    "print(\"Numerical features scaled using StandardScaler.\")"
   ],
   "id": "49b394df10e03a65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## 3. Determine Optimal K (Elbow Method) ðŸ“\n",
    "print(\"Running K-Means for K=1 to K=10 to find the optimal number of clusters...\")\n",
    "\n",
    "inertia = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    # Set a fixed random_state for reproducibility\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto', max_iter=300)\n",
    "    kmeans.fit(store_data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the Elbow Method Results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertia, marker=\"o\", linestyle=\"--\", color='darkcyan')\n",
    "plt.title(\"Elbow Method for Optimal Number of Clusters (K)\", fontsize=14)\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Inertia (Within-Cluster Sum of Squares - WCSS)\")\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"The 'elbow' point is where the decrease in WCSS slows down, suggesting a good trade-off. We will analyze K=3 and K=5.\")\n"
   ],
   "id": "da5e3b85bfb3beeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## 4. K-Means Clustering and Analysis ðŸ’¡\n",
    "\n",
    "# 4a. Clustering with K=3 (Optimal based on typical elbow observation)\n",
    "K_3 = 3\n",
    "print(f\"\\n### K-Means Clustering with K={K_3}\")\n",
    "kmeans_3 = KMeans(n_clusters=K_3, random_state=42, n_init='auto', max_iter=300)\n",
    "store_data[f\"Cluster_{K_3}\"] = kmeans_3.fit_predict(store_data_scaled)\n",
    "\n",
    "# Cluster Analysis (K=3) - Mean of UN-SCALED features for easy interpretation\n",
    "cluster_profiles_3 = store_data.groupby(f\"Cluster_{K_3}\")[clustering_features + [\"StoreType\", \"State\"]].agg({\n",
    "    **{f: 'mean' for f in clustering_features},\n",
    "    \"StoreType\": lambda x: x.mode()[0],\n",
    "    \"State\": lambda x: x.mode()[0],\n",
    "}).round(2)\n",
    "\n",
    "print(f\"\\n--- Cluster Profiles (K={K_3}) ---\")\n",
    "print(tabulate(cluster_profiles_3, headers='keys', tablefmt='pipe', numalign=\"left\"))\n",
    "\n",
    "# Visualization of K=3\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=store_data,\n",
    "    x=\"Sales\",\n",
    "    y=\"Customers\",\n",
    "    hue=f\"Cluster_{K_3}\",\n",
    "    palette=\"viridis\",\n",
    "    s=100,\n",
    ")\n",
    "plt.title(f\"Store Segments (K={K_3}): Avg Sales vs Avg Customers\", fontsize=14)\n",
    "plt.xlabel(\"Average Daily Sales\")\n",
    "plt.ylabel(\"Average Daily Customers\")\n",
    "plt.legend(title=\"Cluster ID\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 4b. Clustering with K=5 (Demonstrating different parameters)\n",
    "K_5 = 5\n",
    "print(f\"\\n### K-Means Clustering with K={K_5} (Alternative Model)\")\n",
    "kmeans_5 = KMeans(n_clusters=K_5, random_state=42, n_init='auto', max_iter=300)\n",
    "store_data[f\"Cluster_{K_5}\"] = kmeans_5.fit_predict(store_data_scaled)\n",
    "\n",
    "# Cluster Analysis (K=5) - Mean of UN-SCALED features\n",
    "cluster_profiles_5 = store_data.groupby(f\"Cluster_{K_5}\")[clustering_features + [\"StoreType\", \"State\"]].agg({\n",
    "    **{f: 'mean' for f in clustering_features},\n",
    "    \"StoreType\": lambda x: x.mode()[0],\n",
    "    \"State\": lambda x: x.mode()[0],\n",
    "}).round(2)\n",
    "\n",
    "print(f\"\\n--- Cluster Profiles (K={K_5}) ---\")\n",
    "print(tabulate(cluster_profiles_5, headers='keys', tablefmt='pipe', numalign=\"left\"))\n",
    "\n",
    "# Visualization of K=5\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=store_data,\n",
    "    x=\"Sales\",\n",
    "    y=\"Customers\",\n",
    "    hue=f\"Cluster_{K_5}\",\n",
    "    palette=\"plasma\",\n",
    "    s=100,\n",
    ")\n",
    "plt.title(f\"Store Segments (K={K_5}): Avg Sales vs Avg Customers\", fontsize=14)\n",
    "plt.xlabel(\"Average Daily Sales\")\n",
    "plt.ylabel(\"Average Daily Customers\")\n",
    "plt.legend(title=\"Cluster ID\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "a9575e4a18519166",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T11:41:59.807489Z",
     "start_time": "2025-11-30T11:41:59.797815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 5. Exporting Results ðŸ“¤\n",
    "\n",
    "print(\"\\n## 5. Exporting Results to CSV Files\")\n",
    "\n",
    "# Create Output directory if it doesn't exist\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"Output\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "output_features = [\"Store\", \"StoreType\", \"Assortment\", \"State\"] + clustering_features + [f\"Cluster_{K_3}\",\n",
    "                                                                                         f\"Cluster_{K_5}\"]\n",
    "\n",
    "# 1. Export the store-level data with cluster assignments\n",
    "store_data_output = store_data[output_features]\n",
    "output_file_1 = OUTPUT_DIR / \"store_clustering_results.csv\"\n",
    "store_data_output.to_csv(output_file_1, index=False)\n",
    "print(f\"-> Exported store-level data to: {output_file_1}\")\n",
    "\n",
    "# 2. Export the cluster profiles (K=3 and K=5)\n",
    "output_file_2 = OUTPUT_DIR / \"cluster_profiles_k3.csv\"\n",
    "output_file_3 = OUTPUT_DIR / \"cluster_profiles_k5.csv\"\n",
    "cluster_profiles_3.to_csv(output_file_2)\n",
    "cluster_profiles_5.to_csv(output_file_3)\n",
    "print(f\"-> Exported cluster profiles to: {output_file_2} and {output_file_3}\")\n",
    "\n",
    "print(\"\\n--- Clustering Analysis Complete. ---\")\n"
   ],
   "id": "5bfaf1f552e2bd0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## 5. Exporting Results to CSV Files\n",
      "Output directory: /Users/altinkelmendi/IdeaProjects/Rossmann/Output\n",
      "-> Exported store-level data to: /Users/altinkelmendi/IdeaProjects/Rossmann/Output/store_clustering_results.csv\n",
      "-> Exported cluster profiles to: /Users/altinkelmendi/IdeaProjects/Rossmann/Output/cluster_profiles_k3.csv and /Users/altinkelmendi/IdeaProjects/Rossmann/Output/cluster_profiles_k5.csv\n",
      "\n",
      "--- Clustering Analysis Complete. ---\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
